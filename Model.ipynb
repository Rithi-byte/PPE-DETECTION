{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1YTjiDYLmhIWomqOmGaoJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rithi-byte/PPE-DETECTION/blob/main/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BnznmaFcO_E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "894f07aa",
        "outputId": "1655ef20-93ae-430b-e6ec-74f40b4af389"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f5ba9251-ff7b-4822-8b1d-451c39cbdc8c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f5ba9251-ff7b-4822-8b1d-451c39cbdc8c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.zip to train.zip\n",
            "User uploaded file \"train.zip\" with length 24420609 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e08e988",
        "outputId": "da05d8ab-c14f-436f-8f7b-ed93343d697c"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Get the name of the uploaded zip file (assuming only one was uploaded and it's 'train.zip')\n",
        "zip_filename = 'train.zip'\n",
        "\n",
        "# Create a ZipFile object and extract all contents\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall('.') # Extract to the current directory\n",
        "\n",
        "print(f\"'{zip_filename}' has been unzipped.\")\n",
        "\n",
        "# List the extracted files to show what was unzipped (optional)\n",
        "print(\"Contents after unzipping:\")\n",
        "for item in os.listdir('.'):\n",
        "    print(item)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'train.zip' has been unzipped.\n",
            "Contents after unzipping:\n",
            ".config\n",
            "train\n",
            "train.zip\n",
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58108ae3"
      },
      "source": [
        "# Task\n",
        "Explore the contents of the \"train\" directory, including subdirectories, to understand the dataset structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e804567c"
      },
      "source": [
        "## Explore Training Data\n",
        "\n",
        "### Subtask:\n",
        "Inspect the contents of the `train` directory (extracted from `train.zip`) to understand the structure of the dataset. This includes identifying image files, annotation formats (if any), and available PPE categories to ensure the data is suitable for object detection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b954218b"
      },
      "source": [
        "**Reasoning**:\n",
        "To understand the dataset's structure, I will use `os.walk` to traverse the `train` directory and its subdirectories, printing the contents of each level.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54bcca52",
        "outputId": "1c096203-f0c6-44f0-865b-fa1162589af7"
      },
      "source": [
        "for dirpath, dirnames, filenames in os.walk('train'):\n",
        "    print(f'Current Directory: {dirpath}')\n",
        "    if dirnames:\n",
        "        print(f'  Subdirectories: {dirnames}')\n",
        "    if filenames:\n",
        "        print(f'  Files: {filenames}')\n",
        "    print()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Directory: train\n",
            "  Subdirectories: ['images', 'labels']\n",
            "\n",
            "Current Directory: train/images\n",
            "  Files: ['00225_jpg.rf.e8373c3ded637a026b4ce0f51a2f181f.jpg', '00274_jpg.rf.0e26dd949e8bb93b097d7dba302461fc.jpg', 'Aitin3205_jpg.rf.3928239086b29168053e8ab386193f52.jpg', '00125_jpg.rf.07c8dd851dfef10c05cd61514a585ba3.jpg', 'Tan0827827_jpg.rf.fa4e4368247443e9714a96d67e7a05fc.jpg', 'ppe_0218_jpg.rf.7557e839d9c985dbe2b79430b69b6348.jpg', 'image_27_jpg.rf.1e9d95b619f6685a642c85af0b32cfe0.jpg', 'AightDuo0294_jpg.rf.172b37718e7a8ac828e135fb961825ec.jpg', 'ppe_0961_jpg.rf.98c4a8fe10718fb9a2844cd64e03c6f2.jpg', 'ppe_0771_jpg.rf.df0ff3556a570dc1a6e2f8e809c2f83e.jpg', '20220721_161927_jpg.rf.51d17739be30a9a5a0c097e74f37a960.jpg', 'Tan0156_jpg.rf.2d3b38cb73b7aa64c2b36390202e7382.jpg', 'construction-worker1_grande_jpg.rf.fa62d0465d2179cab25ea12d37c0482c.jpg', 'KakaoTalk_20221018_001312059_02_jpg.rf.1caa16cccf2ea0973ef231f23336a107.jpg', 'ppe_0274_jpg.rf.9c765f6413e15d0d46e7c149be8826c8.jpg', 'image_150_jpg.rf.05729f2fc36c7435f1c86a687c04e315.jpg', '00095_jpg.rf.692d5fe1fbf646d50159f132cdb1d1bb.jpg', '98_JPG_jpg.rf.a8698c2ac7089a25dc4c618f343ccd81.jpg', '20220721_162848_jpg.rf.1a3724a86c40458c3c6df50f6931b7bc.jpg', 'Aitin2429_jpg.rf.7a8309e1b726996eed34edf4863008aa.jpg', 'image_62_jpg.rf.98056f672eb6a39f0c8c5458639420a1.jpg', 'sic5242_jpg.rf.1c5bf41842d2c380a3a6add05ef5d382.jpg', 'Aitin2742_jpg.rf.6537200822fde884749c08ccaa58ec77.jpg', 'thumb0333_jpg.rf.73587db387d14da731168880c90b9a90.jpg', '20220721_162229_jpg.rf.766fdc6bfe1f6f23dd1876dceb0e5473.jpg', '00107_jpg.rf.f76e374ba462796981cdba9dd06de2e1.jpg', 'Aitin1170_jpg.rf.cd144a3cc2716b0261e9c939bc0a372a.jpg', '59_JPG_jpg.rf.45cb23e0c33ab50d3b6cc0d90eb8e643.jpg', '00254_jpg.rf.323a0b1eb46d0d330b5e2cb82306a8c8.jpg', 'images85_jpg.rf.8df20ddcc81f3b75a148b5ce9721a7c5.jpg', 'KakaoTalk_20221018_001425789_06_jpg.rf.1118e205d00e3002b7f0e57ef8846b46.jpg', 'Video4_278_jpg.rf.9fbb32082ee9b21cff70201068bc3afa.jpg', 'Aitin0937_jpg.rf.5cc6ad7eea6319d2021f64e55516766c.jpg', '109_JPG_jpg.rf.ea3a20a1c08f2266d0672a2895d3bdbc.jpg', 'images-34-_jpg.rf.a2e7a290e78ba81c691444b2bde1b9bb.jpg', 'download-27-_jpg.rf.073ff6cfbfa355fe61bf867082faf963.jpg', '00629_jpg.rf.32f4472018b8d2428d95fed81f860f13.jpg', 'ppe_1188_jpg.rf.df0449d47f7debaf7fdf67f0aaf175af.jpg', 'ppe_1270_jpg.rf.c9a4ff0cdc6969f7276c1eea8a0a18d9.jpg', 'Aitin4095_jpg.rf.be4542770492289674b1f7e1e2ea3ab8.jpg', 'KakaoTalk_20221018_001219713_26_jpg.rf.65d537b411218db182c85379f6f4681e.jpg', 'ppe_1200_jpg.rf.47f160e0631be78ae0c6764931ee7b72.jpg', 'images-2022-07-04T014014_jpg.rf.9f535e16462e848d77256efd50e9fd6c.jpg', 'Aitin2429_jpg.rf.a42011bbfecc38dccf6d35c3c97120c2.jpg', '20220721_162157_jpg.rf.085e779c59d7eb698025a92413caa165.jpg', '20220621_132920_jpg.rf.5f2ad1210369d44059794cdd59b2cc8e.jpg', 'images-34-_jpg.rf.15dde987e90301893be022fabe0a055d.jpg', 'construction-crack-146741_jpg.rf.7c683e60b0673d636175f88cb53bd751.jpg', 'image_95_jpg.rf.c947fb94923989c3d5710ac0aaa198f9.jpg', 'Aitin2331_jpg.rf.e4bad2aed25d52447a85a60dc7ea664c.jpg', 'thumb0454_jpg.rf.0138fa9e35c8740ee06b125b69a5d468.jpg', 'Safety-goggles_6_jpeg_jpg.rf.059a0aeb941474bcf4563cf4537aba30.jpg', 'KakaoTalk_20221018_001425789_06_jpg.rf.fd70da8486d22494235983d8ba375985.jpg', '00212_jpg.rf.582a67c0ca4eaf8c1e37fafb09e51bdb.jpg', 'Aitin2428_jpg.rf.0a77bb4a80dd6063db7c21e3fbe90be2.jpg', 'images-2022-07-04T013058_jpg.rf.b47ed0ecb91e10fb3f99a127ed9c8be0.jpg', 'Aitin3852_jpg.rf.c1d10b6023afca6c2acbbd3b0b3b54ff.jpg', '98_JPG_jpg.rf.0fdb917ec7108fe07a65a13b052737ef.jpg', '00254_jpg.rf.9f2047c0496e33024325272f6e5d077c.jpg', 'Image_ipg-1381-_jpg.rf.4a975bdc4f0c2134682f14872abc45d3.jpg', 'images-2022-07-04T013045_jpg.rf.35571925018f76d664af8c047a6f8c11.jpg', '00182_jpg.rf.3b6a7898b9e19c9a733c7a7b125d6fbf.jpg', 'KakaoTalk_20221018_001219713_26_jpg.rf.0d4be2008723e1ce3f81ba6029a62026.jpg', 'KakaoTalk_20221018_001219713_18_jpg.rf.df53f846897687b74cff79113e68b396.jpg', 'gettyimages-632170387-612x612_jpg.rf.b0fa16091503e7863ad7d031fa2f9a79.jpg', 'images-2022-07-04T013958_jpg.rf.3c2266829cb0473857b18b78f4fe5c84.jpg', 'download-27-_jpg.rf.412bf5a92170ccd5969c61b1387870e4.jpg', '59_JPG_jpg.rf.4ccc8803d49ae18600038d2ca9c57a03.jpg', 'ppe_0873_jpg.rf.67517fd098a86b5aca0a119d0ce30b27.jpg', 'ppe_0354_jpg.rf.48900e87fd6bf4ad9940e23b4141ef32.jpg', 'AightSird0243_jpg.rf.2dcb62e9d01199384d933c54d3576fa5.jpg', 'Image_ipg-1381-_jpg.rf.1d245a1aaa5ff139ce9e9806c11a78f1.jpg', '20220721_162157_jpg.rf.c8602cce2fe2ab50fb0dd0740e58a5bc.jpg', 'AightSird0497_jpg.rf.4cd157548f9c76b8eabb9bb1fc81e41a.jpg', 'ppe_0140_jpg.rf.4e92fd6476f50f9587336ec0749aa0a3.jpg', 'Aitin1788_jpg.rf.14375e679fbdaf0a9133a464632024d1.jpg', '20220721_161923_jpg.rf.da58dda6dfd4e9adb022c8643db2ae67.jpg', 'li-lin-rZcC9J5ZT6I-unsplash_jpg.rf.2bfb636ce2379bede4435ea4efd355c4.jpg', 'ppe_0274_jpg.rf.4e88d83fcd9beb25fe34bcd73a4771fa.jpg', 'images-31-_jpg.rf.6266e935fec0684c1e1c5b9c87ead44e.jpg', 'Aitin2428_jpg.rf.a3ccd7c7ffb9b3c2435c62dc2a7f7ac1.jpg', 'KakaoTalk_20221018_001312059_02_jpg.rf.4a1aec20a6dda4a59ed8478ed7a65502.jpg', '00629_jpg.rf.518aa45885c1f374fe7ee3b73735fb11.jpg', 'KakaoTalk_20221018_001425789_06_jpg.rf.6c3becab24575677e198bc091df18756.jpg', 'KakaoTalk_20221018_001219713_18_jpg.rf.0c2745e3f57591cb9d799b0105731d53.jpg', 'KakaoTalk_20221018_001219713_26_jpg.rf.2f824438cf499b82245ff8d03214a5f0.jpg', '52_JPG_jpg.rf.7fd375488d4c8a73b7da19a8935e239b.jpg', '00422_jpg.rf.c559a73c053de4c06b633cac22493a2a.jpg', 'KakaoTalk_20221018_001219713_26_jpg.rf.d7147d42ef4223e533e5a5ceb56b0fe2.jpg', 'Aitin0036_jpg.rf.ab2a416e5e29f3292237dd42988ba7ba.jpg', 'ppe_0925_jpg.rf.26cade37d10a2a415996898635608244.jpg', 'KakaoTalk_20221018_001312059_02_jpg.rf.a363867025f4034179a7d1920c043fec.jpg', '00014_jpg.rf.dbd86f7597d25f36766dd2bd1ab6095e.jpg', 'ppe_0961_jpg.rf.a17acf1a82a374f05eb5a178219382ba.jpg', 'AightOne0967_jpg.rf.94417c0dfa39f6e604739c009bcf4384.jpg', 'Image_ipg-2083-_jpg.rf.2de92817dc35ba6a98870b0a5c95ff4c.jpg', 'image_95_jpg.rf.720896090f438b98bc49ebcf22693c22.jpg', 'image_150_jpg.rf.6b9ea08e9f5327613f7d63061009b94e.jpg', 'Aitin2331_jpg.rf.8c713e0cf8abc43a18dcbd618aa420cf.jpg', 'Aitin0150_jpg.rf.948cff79061a6936e6650d517f14c695.jpg', '00035_jpg.rf.042678756a8c86312e68e1b72f5795eb.jpg', '00100_jpg.rf.ec5e9f7304e6d6f6945c3a44b6ef5911.jpg', 'ppe_0153_jpg.rf.f8fb327df38524164a96bd161e0f3f86.jpg', 'images-2022-07-04T013948_jpg.rf.4d47baafd37588185bfd8d15c5c14c09.jpg', 'KakaoTalk_20221018_001219713_18_jpg.rf.c77813984de67d78ec4128ad81a35408.jpg', 'ppe_0648_jpg.rf.22f1ebf1bb4eb956c77c7e3666ada6c4.jpg', 'Tan0296_jpg.rf.ed1f850de850eccc8fcecf37a0c34218.jpg', 'ppe_1270_jpg.rf.c62e22b71eb690a82c23f5d3f2a608ee.jpg', 'ppe_0771_jpg.rf.ff35c81b2fe33757a98b7bb1cb615b76.jpg', 'Tan0745745_jpg.rf.f09126526d369aab7fbebe2c86b556fc.jpg', '00100_jpg.rf.e10bd910a5b938d51e9c154de67d3b58.jpg', 'images-2022-07-04T013435_jpg.rf.029c19cb8e930c27feee50079cca391c.jpg', 'AightOne0641_jpg.rf.686b461e1c05232b1104341c7025c80a.jpg', 'Aitin0937_jpg.rf.bdd27201fa8794bfceb2a32819f53db8.jpg', 'KakaoTalk_20221018_001219713_18_jpg.rf.8a7ac165a73d630b3767132f6be8afdc.jpg', 'Tan0972972_jpg.rf.5a20a9d9ca3e9c56e5e76c955de979b0.jpg', '00100_jpg.rf.c60444aedbc8d29526e7fc4e02f687f6.jpg', 'images237_jpg.rf.e99e7a33ef538eb19181f05a750da973.jpg', 'Tan0827827_jpg.rf.35672786a2d5aa13094b144c898fb26c.jpg', 'Tan0207_jpg.rf.c8da58d1a2d3903fec1d75efc3434699.jpg', 'Tan0125_jpg.rf.41d74d5b03c33dc95c3a0b3e7e24ca8e.jpg', 'AightOne0167_jpg.rf.f4a21f0051b624c0def7de41b1271abd.jpg', 'ppe_0274_jpg.rf.2f4caa8ee13f136583c9e74db20d4ba2.jpg', 'gettyimages-632170387-612x612_jpg.rf.bd8ec50ff56a126da57e7443030fa513.jpg', 'KakaoTalk_20221018_001219713_26_jpg.rf.383d6b97778bfb2091996e974d1147e4.jpg', 'image_153-1-_jpg.rf.d122cac0ba1d00d9932deb34ae7f0fc0.jpg', '20220621_132901_jpg.rf.df1e285fad26c8f077fbe06c74b575d4.jpg', '00060_jpg.rf.8454996f2ead80567c7b1b1c0777e005.jpg', 'AightOne0167_jpg.rf.786a0750e54a12c2637ab7d249d7fa90.jpg', 'image_70_jpg.rf.786b9f2d4d5fb89e9118282e36188066.jpg', 'images-34-_jpg.rf.3726101c0f8ccd2106118cd73b390b63.jpg', 'Tan0933933_jpg.rf.59a90b5e8898afdc80a0fc8017e14723.jpg', 'Aitin0937_jpg.rf.f194aae66584fde8b42b21ff150c45eb.jpg', 'image_62_jpg.rf.bcb3f6081938b452160cbbad58bcf136.jpg', 'thumb2443_jpg.rf.daa0f243b3b71e056032de2b97b99906.jpg', '47_JPG_jpg.rf.56b8bc3f444d8febc5f320f9d89cb154.jpg', '00056_jpg.rf.4aa4f4a6efc561fa6fd290e052de1790.jpg', 'Tan0296_jpg.rf.af256cd19aabd013486118d7558d9976.jpg', 'download-1-_jpeg_jpg.rf.cbb901f4f1952b0df1a07137bb497f36.jpg', '59_JPG_jpg.rf.195d5a98ebfb4959267c5222b7606189.jpg', '00125_jpg.rf.e35c73dcb90f69ed1666c140649eb161.jpg', '00350_jpg.rf.8b6c3d41ca69e29b2833be7c7c4addb1.jpg', 'Image_ipg-1958-_jpg.rf.793272fdb111d5d0ce79e9561294b5f0.jpg', 'Tan0150150_jpg.rf.21dbcae71da2ed5ed22d8fae7fd70c67.jpg', 'images-50-_jpg.rf.09d7ec3e8f9d1d0bd56739d09c021e3a.jpg', '00274_jpg.rf.19d58fa6b87d3e7f6a9ef095ad6df491.jpg', 'images237_jpg.rf.3b667f1244a25e2108065d6552234a60.jpg', 'images-2022-07-04T013045_jpg.rf.bc83516215179e966c6c84330fc0c49b.jpg', '47_JPG_jpg.rf.48de2b1e1c403df10484036d262229ce.jpg', '105_JPG_jpg.rf.83a75069633167494f7140cb320b56fa.jpg', 'image_151_jpg.rf.1b6d5befdeeb80abc97aa6c87e1d2cdd.jpg', 'ThavantinDuo0848_jpg.rf.31299026d2dad5ff748fb8d912043789.jpg', 'Video4_278_jpg.rf.6995bfabcbd109105f6c015ebb31d2f9.jpg', 'ppe_0311_jpg.rf.dcc7dae5265f5ccbcc39ac3b09914836.jpg', 'images-29-_jpg.rf.3fd00267a41e44a49aed9cbed48c364b.jpg', 'KakaoTalk_20221018_001219713_24_jpg.rf.b670fc8eccfd83b8919bc05777a9eb67.jpg', 'ppe_0218_jpg.rf.dd36676e179a6be3830ea84301f490c3.jpg', 'images49_jpg.rf.8bd6887e8033dde2d492684ac95ec4aa.jpg', 'gettyimages-650169013-612x612_jpg.rf.ca1605dba29be6920384d2009441d9bc.jpg', '20220721_162229_jpg.rf.e5ce519cce9cfd36ab896365f4a76201.jpg', 'ppe_0218_jpg.rf.4151a15d1a2cf7c16bc1583e29d4d354.jpg', '20220621_132901_jpg.rf.09a3d2f28db56084d8cb295341c72f11.jpg', 'AightOne0317_jpg.rf.61eb8e8bf70b783b89f67669a756ae49.jpg', 'AightOne0317_jpg.rf.a8b7009c8a775f65bd6aa2b94318e2f8.jpg', '9417812-serious-male-construction-worker-in-safety-vest-and-hard-hat_jpg.rf.eb6d9f412b7e258ee0452cd5a6647849.jpg', 'construction-worker1_grande_jpg.rf.26cd85ce6ce87da7e06a30a3e92226ca.jpg', 'Tan0125_jpg.rf.7894b78acb26e5e0cd36931ee6c01cb2.jpg', 'Tan0207_jpg.rf.613afbcdc363cec54734b2ccfc76ab5b.jpg', 'Image_ipg-1958-_jpg.rf.40da7e26144bd8ea7e749ec5ff652127.jpg', '00113_jpg.rf.40d8a387c2b2d1bbf9ae7c47a95ccfe8.jpg', 'images-2022-07-04T013955_jpg.rf.024b4a838bdff435ba2e705ea6556307.jpg', 'Aitin0942_jpg.rf.168851721482bfc94865e8f6c5ea073f.jpg', 'KakaoTalk_20221018_001219713_16_jpg.rf.1e4fc4df1a101adc5811737b6566a93e.jpg', '00176_jpg.rf.8e1519a453341ef1dfb5b981bee58400.jpg', 'Aitin0150_jpg.rf.b212d98d65ab9b2ddae725cae2d73e0e.jpg', 'Tan0150150_jpg.rf.fd522568c2efa3d7d996f9968c53a09a.jpg', 'images-29-_jpg.rf.3f2242320959d96fda0e52d8b41f8120.jpg', 'image_150_jpg.rf.274aa0c4b37dcac2a59f60e98139031c.jpg', '20220721_161927_jpg.rf.16969a5b268932f601da501337901c9d.jpg', '00096_jpg.rf.6e33e05e149f4ef82004a85a085b9e36.jpg', '51_JPG_jpg.rf.af2c775d8ef36e12b28bdcc365ee1eb7.jpg', '101_JPG_jpg.rf.92641dc3bfda90109f2d2a3f5be95781.jpg', 'Aitin0586_jpg.rf.f0c8dd6604a12970fb9260673ba61558.jpg', '00350_jpg.rf.b63b53649c0c6fcbf7179f333ace8b8a.jpg', 'Tan0827827_jpg.rf.1dbf322ef9aa345169e6326fdcb60899.jpg', 'image_53_jpg.rf.e87c51dbd394de303a8b89ed5818b4f6.jpg', 'image_153_jpg.rf.43826bc683f4fb8a60765b5851783ffb.jpg', 'AightOne0967_jpg.rf.db7ec878e78ac9fa467ce6d13d2c4768.jpg', '20220721_162229_jpg.rf.1810436820695da73550511886a3e412.jpg', 'Tan0082_jpg.rf.a10134d5c23b8f3fcd0daf608f5ea01f.jpg', 'ppe_0352_jpg.rf.1b858fb02cb1133a7a756a3bf8dd5695.jpg', 'ppe_0352_jpg.rf.efdbe48341e3309f1d41e9851ea0c85e.jpg', 'image_96_jpg.rf.bc4aad3eda17025822c0670ac3ba95e6.jpg', 'Aitin0937_jpg.rf.6f66b3b3451f8bb02078e5250b9e4479.jpg', 'thumb2443_jpg.rf.0054a22e31d5a195f1ed3ddfb17b28d4.jpg', 'Ear-protection_5_jpeg_jpg.rf.9845f4b0320e80110cc691439f3015fe.jpg', 'images-30-_jpg.rf.d91b215c164f9f97a28af45e766cbd35.jpg', '51_JPG_jpg.rf.b5ba2155fc0fbd0925ade773a0d440ed.jpg', 'thumb0453_jpg.rf.36ce3cfc8e46c4417d957e47aae37489.jpg', '101_JPG_jpg.rf.4447a998927b2e7c192b474ff1078986.jpg', 'images49_jpg.rf.1fdd948f82293efb0cdb225e5f42ffc7.jpg', 'ppe_0546_jpg.rf.4a1ff5db7d4db82b9831f11414a969e4.jpg', 'images63_jpg.rf.19a07d3f28c13e001482edae2bc46cdb.jpg', 'image_4_jpg.rf.092619efbe3c571e65edf86ef1685c2c.jpg', 'image_62_jpg.rf.732950990a32fb5b0a0feacd95772ba4.jpg', 'Aitin3205_jpg.rf.c59fdeaf58fc8c8a8e6e1232d384e62d.jpg', 'Ear-protection_5_jpeg_jpg.rf.f1536551ded60572c53c7123912ab34d.jpg', 'AightOne0029_jpg.rf.fc5848ae2e8c4904c90be6e7af084d24.jpg', 'ppe_0636_jpg.rf.10a35cf4414b4962b4257e5d19a67f3d.jpg', 'images-2022-07-04T013441_jpg.rf.3339b26fc8648fbdb6ccd66f66aaa1f0.jpg', 'Aitin1410_jpg.rf.6631e8d7ec8fe9c5bfa59f1842883a02.jpg', '00035_jpg.rf.dcec12c11100325ce028dbb19311e70c.jpg', '68_JPG_jpg.rf.19114e5270a0df784b3da152cdebd4c5.jpg', 'images-50-_jpg.rf.fdc7799b5258e58d9dda054332d99726.jpg', 'Tan0500500_jpg.rf.522d1a09ff8c0f3e81d56086fe4f519c.jpg', 'Tan0914914_jpg.rf.f62f11d4fcf5b1f7972b58549fde53d9.jpg', 'sic2817_jpg.rf.5d820c9950ff1895cda46fb58ccb38dc.jpg', 'Aitin2428_jpg.rf.ca5ce4904cff178653a5294666386d3b.jpg', '00119_jpg.rf.13a292303113e279bee57d48d14f9f7a.jpg', 'image_62_jpg.rf.2c1cc95b22505d1d64748edb2d1c218c.jpg', 'images85_jpg.rf.87da82ea5d90067fb93c3ad2c2bb8d8c.jpg', 'ppe_0153_jpg.rf.bb4cb7f5d855ba551bfda8f8f67d2969.jpg', 'thumb2444_jpg.rf.bd8ec5fe10c18ded4fc2df6390e99dc7.jpg', 'ppe_1211_jpg.rf.b9f30ee8b3dfed7b21ff774911134456.jpg', '00350_jpg.rf.2cf99f822e01a5c1fb00c0799adab7b8.jpg', 'ppe_0153_jpg.rf.622b665fe88b4c797fd44e9ad546bb19.jpg', '95_JPG_jpg.rf.7d3c2141f05e63c17837a3db8c6eff4a.jpg', 'AightOne0832_jpg.rf.5fc534a2e5d81a873268ccb48207657e.jpg', '51_JPG_jpg.rf.7c2bbabb289a4489b8cb6bd74fec4711.jpg', 'Aitin3205_jpg.rf.e129b1ae82ed5231b6fc45de3dbd6b9b.jpg', 'images-2022-07-04T013055-1-_jpg.rf.91c723a41b8b93d9f99b81e9dd5301e2.jpg', '00014_jpg.rf.9a47c0ead79bdaffba262e121a7b3863.jpg', 'Tan0500500_jpg.rf.cf0577ebe9dc4603292520c95940afd6.jpg', 'Aitin3205_jpg.rf.df7cb7328854de9fc31cd6d21db42feb.jpg', 'images-2022-07-04T013506_jpg.rf.616716d73e83d2f6e6d7ba61ff772889.jpg', '59_JPG_jpg.rf.05468bcba1a284cdf51667e339187519.jpg', 'AightDuo0193_jpg.rf.f29bb507697d23ec7db0c24d0ee57876.jpg', '20220721_161927_jpg.rf.e379d350eb5b76a29828671028e72348.jpg', '52_JPG_jpg.rf.6d43eb0c8861c5cfc1909f835b89c654.jpg', '00350_jpg.rf.131ac7d757335bdad7bad5a450caea44.jpg', 'images-2022-07-04T013055-1-_jpg.rf.0c3d860e94d5c37e836e27502c59819e.jpg', '00137_jpg.rf.c7b774e79fe7979768ba9a1c985c56fe.jpg', '20220721_162157_jpg.rf.5a1b24822fc2c77e3e18f624c2289996.jpg', 'ppe_0302_jpg.rf.1fcac53b74f17bf47cfaae3b3099039c.jpg', '00121_jpg.rf.22d8e7ce179785da11044cd943ac1c33.jpg', '00236_jpg.rf.27fbdb231773faf7dd7e9d535a289e5f.jpg', '00422_jpg.rf.66a2ca8bc031f46e62517105f3fa0b14.jpg', 'images-68-_jpg.rf.fc3ad924e0fdd0028e80a1eec0bdf063.jpg', '20220721_162202_jpg.rf.7052a8cac08be49258538095f9ddf0e6.jpg', 'image_150_jpg.rf.f3f481634131bddcf1237560122f18ff.jpg', '9417812-serious-male-construction-worker-in-safety-vest-and-hard-hat_jpg.rf.73fe41f3bc70aaa4c741b55cd9c7458d.jpg', 'Aitin4095_jpg.rf.8d71f96a281985f310bdcf1fb6e7e82d.jpg', '00176_jpg.rf.9de173b652c4eb79ad748c9b82a18484.jpg', '00087_jpg.rf.72c6141b1e29512b63e0bc4e6ef55082.jpg', 'image_53_jpg.rf.9118fa1e0faee7324b3df9e5c1230123.jpg', 'ppe_1211_jpg.rf.366a0b949c2f475bca37312b0ec67dde.jpg', 'thumb2444_jpg.rf.2a37a708eb54af5d960b3ef30a890ad2.jpg', '20220721_162157_jpg.rf.2decaa9a04756ff87fbda0f5ee81eb93.jpg', 'ppe_1207_jpg.rf.cab304c988ef29a8fde0125a17d2f97d.jpg', 'thumb2435_jpg.rf.e0e11502cfbcebcff8f20db411c9ae77.jpg', 'image_97_jpg.rf.f586c144181b016e84d3f09a986e5706.jpg', '00085_jpg.rf.0ee13de379a1dce820e28f19a3166e6c.jpg', 'KakaoTalk_20221018_001425789_06_jpg.rf.e01fee158aa601df58ca003984ee2604.jpg', '00014_jpg.rf.b79345aaed5b59620b7e311442b1984e.jpg', 'Aitin2737_jpg.rf.5b05c5b3c482d0cf094b4d8d3b7cdcc9.jpg', '00486_jpg.rf.fa2cc8e526524d25b119b22eed5eee35.jpg', 'li-lin-rZcC9J5ZT6I-unsplash_jpg.rf.84cd09fca336a1f0c84a26ce6e969154.jpg', 'download-1-_jpeg_jpg.rf.dfcf64705d4ef9f33d336f571c958aad.jpg', 'ppe_1188_jpg.rf.8b9aeede6d651f9312ab9d63f0be5454.jpg', 'Worker-with-safety-glasses_6_jpeg_jpg.rf.27d9fce9c992ced8b97d5b2a15376e2b.jpg', 'ppe_0374_jpg.rf.3470c9435b2aad843c0c085e977c1698.jpg', 'images-56-_jpg.rf.27debeb6890180e3335f3ffbbbc078ab.jpg', 'image_52_jpg.rf.80363fcf1900dba17a198676d1373694.jpg', 'KakaoTalk_20221018_001425789_06_jpg.rf.48c5650b8819d82ede3fd542f631d997.jpg', 'AightSird0207_jpg.rf.13c68e33e8172f3adadc561c4c7c6179.jpg', '20220721_161927_jpg.rf.42dc35f820b8fcbceae7f011863797ae.jpg', 'images-2022-07-04T013101_jpg.rf.4e4990189326b2785c15ba10647c02ee.jpg', 'Aitin0150_jpg.rf.2c6573172ff4e6808b6327063e3b028d.jpg', '20220721_162848_jpg.rf.8c29c48ec64b8ed5a0d50dcdaf3b498e.jpg', 'construction-worker1_grande_jpg.rf.741bf338a483f65b91f84c3d3b62f54c.jpg', '4843_RolePlay_ConstructionWorker_BoyGirl_jpg.rf.30a570e7ec2038ba2d30e2711e1fbe69.jpg', '9417812-serious-male-construction-worker-in-safety-vest-and-hard-hat_jpg.rf.fbde6ada635db7156dbbbf90276e1888.jpg', 'download-3-_jpeg_jpg.rf.02a726ce0f147035d40a68e44dec73c1.jpg', 'ppe_1328_jpg.rf.19aea035201814652a74bbe18ee62a5a.jpg', 'Tan0365365_jpg.rf.cfcc704b715217660de3c37bec0c3b28.jpg', '00122_jpg.rf.9036cddcbb176e79f21e258fc57e2e38.jpg', 'image_150_jpg.rf.70c4e25ad117ff918df6e39179fb050a.jpg', '00236_jpg.rf.8b9218814fba6ab93cfd814cbb45f1d4.jpg', 'ppe_0961_jpg.rf.de9c3d0700fef07c66197ea08786b6ef.jpg', '00095_jpg.rf.6204f8dac034945e959926a16a3444df.jpg', 'Tan004343_jpg.rf.6eff7220fea2f1afcf12f5815145099d.jpg', 'images-2022-07-04T013203_jpg.rf.2d1fb5520eac550f50ae602753f925c4.jpg', '00091_jpg.rf.ae53204a80b27e781785c047a2a294ab.jpg', '00271_jpg.rf.7fed247288f1b621bc9aac1c72808a73.jpg', 'ppe_0896_jpg.rf.1866b6c4000aa44d1e48ac9048af1002.jpg', 'Tan0933933_jpg.rf.77124eca9469073eec831a7d8713adc0.jpg', 'Aitin2742_jpg.rf.1c67afc3b3ba20c0047e3a9a7407558f.jpg', 'download-27-_jpg.rf.f1c880da5f92b8af159da8af4c9f6332.jpg', 'ppe_0648_jpg.rf.bc36e78429000d3652023e7eb25734d5.jpg', 'image_4_jpg.rf.6ea6d9e745ba57a8b249e4c6347df03e.jpg', 'image_27_jpg.rf.448b6255132790403da80ea27f03ff21.jpg', '00126_jpg.rf.1971dcdda113ecd640024705b3eda7c5.jpg', 'thumb0454_jpg.rf.0ccb33330934d69e68634a8b98185991.jpg', 'ppe_1188_jpg.rf.3a7e953cbdab6f338ade9e5e6608c762.jpg', 'AightOne0572_jpg.rf.078a25873a7229bb02d4ba0f17201ec4.jpg', 'Aitin2429_jpg.rf.10d2644e0bad53d709d4b85c125c8767.jpg', 'images-2022-07-04T013045_jpg.rf.074fe39fec5ae80af9915238d2b07bc4.jpg', '00060_jpg.rf.cd4b0a089c92f916228e9cd0143447a9.jpg', 'Image_ipg-1958-_jpg.rf.969659e7bd4e04d64241c41161ae8b94.jpg', '20220721_162157_jpg.rf.0cf80eaba575f140e4b84c6767d03b48.jpg', 'AightSird0497_jpg.rf.5e5b1377414b5d5baa722263713933d3.jpg', 'Ear-protection_14_jpeg_jpg.rf.a955f51831248185e700875c6177db0d.jpg', 'AightOne0138_jpg.rf.f6009d2596a77936f8ab5804508b787e.jpg', 'Worker-with-safety-glasses_11_jpeg_jpg.rf.f956d896fd68d70cb4c7bb5bf92450f3.jpg', 'images-2022-07-04T013506_jpg.rf.aa2756e976ef5969a9c162ba605e3891.jpg', 'Tan0365365_jpg.rf.00bd80dfa6579f96ff7bd1c312d4913a.jpg', 'Video4_273_jpg.rf.ca58e5f6f92ec6750cfc10bab8794e8f.jpg', 'ppe_0546_jpg.rf.f8744f11fb512210682f52436140db12.jpg', 'KakaoTalk_20221018_001219713_15_jpg.rf.c513c77b559cda5cb774a87cb962ff12.jpg', 'download-3-_jpeg_jpg.rf.dc6be9c641b70e1f43fd9b688662c2d3.jpg', 'Aitin3626_jpg.rf.3e8a073533b2f5d6824a0004c9ae648d.jpg', '00011_jpg.rf.151597342c6d822a56c3ccc8e0969595.jpg', 'images-2022-07-04T013435_jpg.rf.043ffca2a6d77842bf4786a5918eb106.jpg', 'image_96_jpg.rf.cda50f00bc9986728eab50ed5235346e.jpg', 'AightSird0542_jpg.rf.da9a1622ee4d4b4beacfbdba236e0a42.jpg', 'Safety-goggles_8_jpeg_jpg.rf.4879f6c0078d9248bf63c4de1c0e6588.jpg', 'image_150_jpg.rf.b0d378d5a9cb89b4f7b0326f417cfb9f.jpg', 'image_96_jpg.rf.d9cc005bf6a21670ce2c298617c2daf7.jpg', 'Tan0639639_jpg.rf.33614f97b2046289c51f459367ba243e.jpg', 'ppe_0311_jpg.rf.4fc6d41663753277688d4742245ee381.jpg', 'Ear-protection_14_jpeg_jpg.rf.ae85305eed761cb48450a20b2cf1636f.jpg', 'ThavantinDuo0848_jpg.rf.ec43bf91488eb30fc778fcd6effe3cde.jpg', 'gettyimages-1036354066-612x612_jpg.rf.15d8e6522e649407b2f50f539e168bc9.jpg', '00004_jpg.rf.ceaaa39b5ce1bc929b07b407d6860953.jpg', 'Video4_281_jpg.rf.53bd3cd9e0802e2bbc5bb12dd7e90a0e.jpg', '00271_jpg.rf.5b01646054d005e47fa45b1601b8621f.jpg', 'Aitin2742_jpg.rf.e1cfc480a724af4d313c3112295995c8.jpg', '20220621_132901_jpg.rf.0f8a77ddc6f933891a8c8a31bc67bd21.jpg', 'AightSird0207_jpg.rf.b7e351c9a0dd9461a0f27ad3507d837b.jpg', '20220721_162229_jpg.rf.0294fbbf760fbbe7ba272e36b9b075c0.jpg', 'Aitin0907_jpg.rf.1a037959fa8968290faa885fce21e6af.jpg', '20220721_162157_jpg.rf.44be624089dc6a2910370bc096f8453e.jpg', 'Aitin0907_jpg.rf.5e7cc581536dcead9819fecf37b9117f.jpg', 'Aitin0001_jpg.rf.642639bf88b1b4ae53b3e14bdbb10d5e.jpg', 'AightSird0120_jpg.rf.7366bc9127be7ff1583a4668ccbbbf58.jpg', '00096_jpg.rf.cd6577a7965f082c447dd42552134f7d.jpg', 'ppe_0140_jpg.rf.ca1d7a3bbb9abbe4455e96748500e7bf.jpg', '00087_jpg.rf.377cf21010d81d8a1e86d8a7b26de620.jpg', 'Aitin0036_jpg.rf.b20d74d6e8fd5a1a5f7a4a36f9e47788.jpg', '00113_jpg.rf.68014fc1cf92f18b2379df974c05d9a6.jpg', 'ppe_0374_jpg.rf.389e8e41774b83505767ab740cda17e9.jpg', 'images-50-_jpg.rf.2bafbe895fc3eade63418522ea0f1de5.jpg', 'KakaoTalk_20221018_001219713_15_jpg.rf.a754840622fd5b7ea2df197246df0424.jpg', '00041_jpg.rf.c6145030654ed36c73550d30f673fd99.jpg', 'AightOne0873_jpg.rf.4edbb3854e1cef79831d893a37c06316.jpg', 'download-1-_jpeg_jpg.rf.4874d5fcfe8e2965fdd60c5951ab4715.jpg', 'images-2022-07-04T013042_jpg.rf.9ad8d195000230ae98e85857c59a3ca1.jpg', '95_JPG_jpg.rf.0a4d2e118896be9d806db8ddb19011e2.jpg', 'Aitin2331_jpg.rf.a5d212d2d01abe198f11ca0ee5e953ab.jpg', 'image_96_jpg.rf.65b6989907a4a7e9027df70b05c5af58.jpg', 'images-2022-07-04T012941-1-_jpg.rf.f91fe6bf0e743083d428d9612e447336.jpg', 'Video4_274_jpg.rf.4963d1092721b6f8d76de470aa76259d.jpg', 'image_62_jpg.rf.abf39e35d581bdb1962c99b1ec28ad62.jpg', 'KakaoTalk_20221018_001312059_02_jpg.rf.3ea3d9d07b5abfa74bbb8060ae55ea71.jpg', 'ppe_0307_jpg.rf.777f894691af83e13da24cc2026398c2.jpg', '00121_jpg.rf.e2569f1c2c5587f9bc623354ae8e6cb3.jpg', 'image_151_jpg.rf.607314ac1b165ba92eb83e3047d3a01b.jpg', 'Aitin0586_jpg.rf.89456c6a74d013d14348d4c75855c9f8.jpg']\n",
            "\n",
            "Current Directory: train/labels\n",
            "  Files: ['ThavantinDuo0848_jpg.rf.31299026d2dad5ff748fb8d912043789.txt', 'image_151_jpg.rf.1b6d5befdeeb80abc97aa6c87e1d2cdd.txt', '00056_jpg.rf.4aa4f4a6efc561fa6fd290e052de1790.txt', '59_JPG_jpg.rf.195d5a98ebfb4959267c5222b7606189.txt', 'KakaoTalk_20221018_001219713_15_jpg.rf.a754840622fd5b7ea2df197246df0424.txt', 'images-50-_jpg.rf.fdc7799b5258e58d9dda054332d99726.txt', '9417812-serious-male-construction-worker-in-safety-vest-and-hard-hat_jpg.rf.eb6d9f412b7e258ee0452cd5a6647849.txt', '00095_jpg.rf.6204f8dac034945e959926a16a3444df.txt', 'AightOne0167_jpg.rf.786a0750e54a12c2637ab7d249d7fa90.txt', 'Aitin1410_jpg.rf.6631e8d7ec8fe9c5bfa59f1842883a02.txt', 'images-2022-07-04T013506_jpg.rf.aa2756e976ef5969a9c162ba605e3891.txt', '51_JPG_jpg.rf.7c2bbabb289a4489b8cb6bd74fec4711.txt', 'Aitin1170_jpg.rf.cd144a3cc2716b0261e9c939bc0a372a.txt', 'KakaoTalk_20221018_001219713_26_jpg.rf.d7147d42ef4223e533e5a5ceb56b0fe2.txt', 'image_150_jpg.rf.70c4e25ad117ff918df6e39179fb050a.txt', 'ppe_0274_jpg.rf.2f4caa8ee13f136583c9e74db20d4ba2.txt', '101_JPG_jpg.rf.4447a998927b2e7c192b474ff1078986.txt', '98_JPG_jpg.rf.a8698c2ac7089a25dc4c618f343ccd81.txt', 'image_62_jpg.rf.732950990a32fb5b0a0feacd95772ba4.txt', '00096_jpg.rf.cd6577a7965f082c447dd42552134f7d.txt', 'Aitin0937_jpg.rf.f194aae66584fde8b42b21ff150c45eb.txt', 'Tan0933933_jpg.rf.77124eca9469073eec831a7d8713adc0.txt', 'AightSird0497_jpg.rf.4cd157548f9c76b8eabb9bb1fc81e41a.txt', 'Image_ipg-1958-_jpg.rf.969659e7bd4e04d64241c41161ae8b94.txt', 'Tan0125_jpg.rf.7894b78acb26e5e0cd36931ee6c01cb2.txt', 'Aitin3205_jpg.rf.e129b1ae82ed5231b6fc45de3dbd6b9b.txt', 'KakaoTalk_20221018_001219713_26_jpg.rf.2f824438cf499b82245ff8d03214a5f0.txt', 'Aitin2737_jpg.rf.5b05c5b3c482d0cf094b4d8d3b7cdcc9.txt', '00011_jpg.rf.151597342c6d822a56c3ccc8e0969595.txt', '59_JPG_jpg.rf.4ccc8803d49ae18600038d2ca9c57a03.txt', 'image_150_jpg.rf.f3f481634131bddcf1237560122f18ff.txt', 'Tan0296_jpg.rf.ed1f850de850eccc8fcecf37a0c34218.txt', 'Tan0296_jpg.rf.af256cd19aabd013486118d7558d9976.txt', 'ppe_0352_jpg.rf.1b858fb02cb1133a7a756a3bf8dd5695.txt', 'ppe_0140_jpg.rf.ca1d7a3bbb9abbe4455e96748500e7bf.txt', '52_JPG_jpg.rf.6d43eb0c8861c5cfc1909f835b89c654.txt', 'Aitin0150_jpg.rf.2c6573172ff4e6808b6327063e3b028d.txt', '20220721_162229_jpg.rf.0294fbbf760fbbe7ba272e36b9b075c0.txt', '00035_jpg.rf.dcec12c11100325ce028dbb19311e70c.txt', 'Tan0365365_jpg.rf.cfcc704b715217660de3c37bec0c3b28.txt', 'Aitin0907_jpg.rf.1a037959fa8968290faa885fce21e6af.txt', '00125_jpg.rf.e35c73dcb90f69ed1666c140649eb161.txt', 'thumb0454_jpg.rf.0ccb33330934d69e68634a8b98185991.txt', '59_JPG_jpg.rf.05468bcba1a284cdf51667e339187519.txt', 'images-2022-07-04T013042_jpg.rf.9ad8d195000230ae98e85857c59a3ca1.txt', 'images-29-_jpg.rf.3fd00267a41e44a49aed9cbed48c364b.txt', '00100_jpg.rf.c60444aedbc8d29526e7fc4e02f687f6.txt', 'download-27-_jpg.rf.f1c880da5f92b8af159da8af4c9f6332.txt', 'image_96_jpg.rf.d9cc005bf6a21670ce2c298617c2daf7.txt', 'ppe_0546_jpg.rf.4a1ff5db7d4db82b9831f11414a969e4.txt', '00100_jpg.rf.ec5e9f7304e6d6f6945c3a44b6ef5911.txt', 'KakaoTalk_20221018_001219713_26_jpg.rf.65d537b411218db182c85379f6f4681e.txt', 'Ear-protection_14_jpeg_jpg.rf.ae85305eed761cb48450a20b2cf1636f.txt', 'KakaoTalk_20221018_001312059_02_jpg.rf.1caa16cccf2ea0973ef231f23336a107.txt', '00041_jpg.rf.c6145030654ed36c73550d30f673fd99.txt', 'Aitin0937_jpg.rf.bdd27201fa8794bfceb2a32819f53db8.txt', '20220721_162157_jpg.rf.44be624089dc6a2910370bc096f8453e.txt', '20220721_162157_jpg.rf.085e779c59d7eb698025a92413caa165.txt', 'ppe_0896_jpg.rf.1866b6c4000aa44d1e48ac9048af1002.txt', 'Aitin4095_jpg.rf.8d71f96a281985f310bdcf1fb6e7e82d.txt', 'ppe_0352_jpg.rf.efdbe48341e3309f1d41e9851ea0c85e.txt', 'Safety-goggles_8_jpeg_jpg.rf.4879f6c0078d9248bf63c4de1c0e6588.txt', 'image_95_jpg.rf.720896090f438b98bc49ebcf22693c22.txt', 'Tan0207_jpg.rf.613afbcdc363cec54734b2ccfc76ab5b.txt', '20220721_162157_jpg.rf.c8602cce2fe2ab50fb0dd0740e58a5bc.txt', 'image_150_jpg.rf.05729f2fc36c7435f1c86a687c04e315.txt', 'ppe_0311_jpg.rf.4fc6d41663753277688d4742245ee381.txt', '20220621_132920_jpg.rf.5f2ad1210369d44059794cdd59b2cc8e.txt', '20220721_162229_jpg.rf.1810436820695da73550511886a3e412.txt', '52_JPG_jpg.rf.7fd375488d4c8a73b7da19a8935e239b.txt', '00014_jpg.rf.9a47c0ead79bdaffba262e121a7b3863.txt', 'Aitin0150_jpg.rf.b212d98d65ab9b2ddae725cae2d73e0e.txt', 'image_62_jpg.rf.2c1cc95b22505d1d64748edb2d1c218c.txt', '20220621_132901_jpg.rf.df1e285fad26c8f077fbe06c74b575d4.txt', 'Aitin2429_jpg.rf.a42011bbfecc38dccf6d35c3c97120c2.txt', '00119_jpg.rf.13a292303113e279bee57d48d14f9f7a.txt', 'thumb2444_jpg.rf.2a37a708eb54af5d960b3ef30a890ad2.txt', 'ppe_1270_jpg.rf.c62e22b71eb690a82c23f5d3f2a608ee.txt', 'ppe_0140_jpg.rf.4e92fd6476f50f9587336ec0749aa0a3.txt', 'images-68-_jpg.rf.fc3ad924e0fdd0028e80a1eec0bdf063.txt', '00113_jpg.rf.68014fc1cf92f18b2379df974c05d9a6.txt', 'Tan0500500_jpg.rf.522d1a09ff8c0f3e81d56086fe4f519c.txt', 'ppe_0925_jpg.rf.26cade37d10a2a415996898635608244.txt', 'Image_ipg-1381-_jpg.rf.1d245a1aaa5ff139ce9e9806c11a78f1.txt', 'image_4_jpg.rf.092619efbe3c571e65edf86ef1685c2c.txt', 'AightOne0641_jpg.rf.686b461e1c05232b1104341c7025c80a.txt', 'Aitin2331_jpg.rf.8c713e0cf8abc43a18dcbd618aa420cf.txt', '47_JPG_jpg.rf.56b8bc3f444d8febc5f320f9d89cb154.txt', 'image_52_jpg.rf.80363fcf1900dba17a198676d1373694.txt', 'Tan0933933_jpg.rf.59a90b5e8898afdc80a0fc8017e14723.txt', 'download-1-_jpeg_jpg.rf.dfcf64705d4ef9f33d336f571c958aad.txt', 'images-34-_jpg.rf.a2e7a290e78ba81c691444b2bde1b9bb.txt', 'Video4_278_jpg.rf.6995bfabcbd109105f6c015ebb31d2f9.txt', 'AightOne0873_jpg.rf.4edbb3854e1cef79831d893a37c06316.txt', '00091_jpg.rf.ae53204a80b27e781785c047a2a294ab.txt', 'KakaoTalk_20221018_001312059_02_jpg.rf.a363867025f4034179a7d1920c043fec.txt', 'Aitin3205_jpg.rf.3928239086b29168053e8ab386193f52.txt', 'Aitin3205_jpg.rf.df7cb7328854de9fc31cd6d21db42feb.txt', 'image_153-1-_jpg.rf.d122cac0ba1d00d9932deb34ae7f0fc0.txt', 'download-1-_jpeg_jpg.rf.cbb901f4f1952b0df1a07137bb497f36.txt', '20220621_132901_jpg.rf.0f8a77ddc6f933891a8c8a31bc67bd21.txt', '00422_jpg.rf.c559a73c053de4c06b633cac22493a2a.txt', 'thumb2443_jpg.rf.daa0f243b3b71e056032de2b97b99906.txt', 'AightOne0317_jpg.rf.a8b7009c8a775f65bd6aa2b94318e2f8.txt', '00350_jpg.rf.2cf99f822e01a5c1fb00c0799adab7b8.txt', 'download-3-_jpeg_jpg.rf.dc6be9c641b70e1f43fd9b688662c2d3.txt', 'ppe_0153_jpg.rf.622b665fe88b4c797fd44e9ad546bb19.txt', '00107_jpg.rf.f76e374ba462796981cdba9dd06de2e1.txt', '20220721_161927_jpg.rf.51d17739be30a9a5a0c097e74f37a960.txt', 'KakaoTalk_20221018_001425789_06_jpg.rf.e01fee158aa601df58ca003984ee2604.txt', 'images237_jpg.rf.e99e7a33ef538eb19181f05a750da973.txt', '20220721_162848_jpg.rf.1a3724a86c40458c3c6df50f6931b7bc.txt', 'Image_ipg-1958-_jpg.rf.793272fdb111d5d0ce79e9561294b5f0.txt', 'AightOne0029_jpg.rf.fc5848ae2e8c4904c90be6e7af084d24.txt', '59_JPG_jpg.rf.45cb23e0c33ab50d3b6cc0d90eb8e643.txt', 'Tan0500500_jpg.rf.cf0577ebe9dc4603292520c95940afd6.txt', 'Aitin0586_jpg.rf.89456c6a74d013d14348d4c75855c9f8.txt', 'Tan0125_jpg.rf.41d74d5b03c33dc95c3a0b3e7e24ca8e.txt', 'li-lin-rZcC9J5ZT6I-unsplash_jpg.rf.84cd09fca336a1f0c84a26ce6e969154.txt', 'image_150_jpg.rf.b0d378d5a9cb89b4f7b0326f417cfb9f.txt', 'image_96_jpg.rf.65b6989907a4a7e9027df70b05c5af58.txt', '109_JPG_jpg.rf.ea3a20a1c08f2266d0672a2895d3bdbc.txt', 'Worker-with-safety-glasses_11_jpeg_jpg.rf.f956d896fd68d70cb4c7bb5bf92450f3.txt', 'ppe_0961_jpg.rf.de9c3d0700fef07c66197ea08786b6ef.txt', '00271_jpg.rf.7fed247288f1b621bc9aac1c72808a73.txt', 'Tan004343_jpg.rf.6eff7220fea2f1afcf12f5815145099d.txt', 'ppe_1270_jpg.rf.c9a4ff0cdc6969f7276c1eea8a0a18d9.txt', 'images-2022-07-04T013045_jpg.rf.bc83516215179e966c6c84330fc0c49b.txt', '00176_jpg.rf.8e1519a453341ef1dfb5b981bee58400.txt', 'gettyimages-632170387-612x612_jpg.rf.b0fa16091503e7863ad7d031fa2f9a79.txt', 'Video4_278_jpg.rf.9fbb32082ee9b21cff70201068bc3afa.txt', 'ppe_1200_jpg.rf.47f160e0631be78ae0c6764931ee7b72.txt', 'AightSird0497_jpg.rf.5e5b1377414b5d5baa722263713933d3.txt', 'KakaoTalk_20221018_001219713_26_jpg.rf.383d6b97778bfb2091996e974d1147e4.txt', 'AightOne0967_jpg.rf.db7ec878e78ac9fa467ce6d13d2c4768.txt', 'images-2022-07-04T013435_jpg.rf.029c19cb8e930c27feee50079cca391c.txt', 'ppe_0648_jpg.rf.22f1ebf1bb4eb956c77c7e3666ada6c4.txt', 'Aitin3626_jpg.rf.3e8a073533b2f5d6824a0004c9ae648d.txt', 'AightOne0572_jpg.rf.078a25873a7229bb02d4ba0f17201ec4.txt', 'ppe_1328_jpg.rf.19aea035201814652a74bbe18ee62a5a.txt', 'Tan0156_jpg.rf.2d3b38cb73b7aa64c2b36390202e7382.txt', 'Aitin0907_jpg.rf.5e7cc581536dcead9819fecf37b9117f.txt', '9417812-serious-male-construction-worker-in-safety-vest-and-hard-hat_jpg.rf.fbde6ada635db7156dbbbf90276e1888.txt', '00121_jpg.rf.22d8e7ce179785da11044cd943ac1c33.txt', '47_JPG_jpg.rf.48de2b1e1c403df10484036d262229ce.txt', '00225_jpg.rf.e8373c3ded637a026b4ce0f51a2f181f.txt', 'AightSird0243_jpg.rf.2dcb62e9d01199384d933c54d3576fa5.txt', 'AightOne0967_jpg.rf.94417c0dfa39f6e604739c009bcf4384.txt', 'ppe_1188_jpg.rf.8b9aeede6d651f9312ab9d63f0be5454.txt', 'Aitin2429_jpg.rf.7a8309e1b726996eed34edf4863008aa.txt', 'Aitin0942_jpg.rf.168851721482bfc94865e8f6c5ea073f.txt', 'KakaoTalk_20221018_001219713_18_jpg.rf.8a7ac165a73d630b3767132f6be8afdc.txt', 'KakaoTalk_20221018_001425789_06_jpg.rf.6c3becab24575677e198bc091df18756.txt', 'Aitin0036_jpg.rf.ab2a416e5e29f3292237dd42988ba7ba.txt', 'Tan0150150_jpg.rf.fd522568c2efa3d7d996f9968c53a09a.txt', '00113_jpg.rf.40d8a387c2b2d1bbf9ae7c47a95ccfe8.txt', 'KakaoTalk_20221018_001312059_02_jpg.rf.3ea3d9d07b5abfa74bbb8060ae55ea71.txt', '00087_jpg.rf.72c6141b1e29512b63e0bc4e6ef55082.txt', 'KakaoTalk_20221018_001425789_06_jpg.rf.48c5650b8819d82ede3fd542f631d997.txt', '00254_jpg.rf.323a0b1eb46d0d330b5e2cb82306a8c8.txt', 'images-31-_jpg.rf.6266e935fec0684c1e1c5b9c87ead44e.txt', 'Aitin2429_jpg.rf.10d2644e0bad53d709d4b85c125c8767.txt', 'AightDuo0294_jpg.rf.172b37718e7a8ac828e135fb961825ec.txt', 'Tan0827827_jpg.rf.1dbf322ef9aa345169e6326fdcb60899.txt', 'image_150_jpg.rf.274aa0c4b37dcac2a59f60e98139031c.txt', '00035_jpg.rf.042678756a8c86312e68e1b72f5795eb.txt', 'Aitin2428_jpg.rf.0a77bb4a80dd6063db7c21e3fbe90be2.txt', '20220721_162202_jpg.rf.7052a8cac08be49258538095f9ddf0e6.txt', 'Tan0914914_jpg.rf.f62f11d4fcf5b1f7972b58549fde53d9.txt', 'construction-worker1_grande_jpg.rf.26cd85ce6ce87da7e06a30a3e92226ca.txt', '00182_jpg.rf.3b6a7898b9e19c9a733c7a7b125d6fbf.txt', 'Video4_281_jpg.rf.53bd3cd9e0802e2bbc5bb12dd7e90a0e.txt', 'ppe_1188_jpg.rf.3a7e953cbdab6f338ade9e5e6608c762.txt', 'construction-worker1_grande_jpg.rf.741bf338a483f65b91f84c3d3b62f54c.txt', '20220721_161927_jpg.rf.42dc35f820b8fcbceae7f011863797ae.txt', 'ppe_1188_jpg.rf.df0449d47f7debaf7fdf67f0aaf175af.txt', 'images-2022-07-04T012941-1-_jpg.rf.f91fe6bf0e743083d428d9612e447336.txt', 'images-56-_jpg.rf.27debeb6890180e3335f3ffbbbc078ab.txt', 'images-2022-07-04T013203_jpg.rf.2d1fb5520eac550f50ae602753f925c4.txt', 'Aitin3205_jpg.rf.c59fdeaf58fc8c8a8e6e1232d384e62d.txt', 'AightSird0120_jpg.rf.7366bc9127be7ff1583a4668ccbbbf58.txt', 'KakaoTalk_20221018_001219713_18_jpg.rf.df53f846897687b74cff79113e68b396.txt', '105_JPG_jpg.rf.83a75069633167494f7140cb320b56fa.txt', 'image_62_jpg.rf.98056f672eb6a39f0c8c5458639420a1.txt', 'Aitin2331_jpg.rf.e4bad2aed25d52447a85a60dc7ea664c.txt', 'images-2022-07-04T013058_jpg.rf.b47ed0ecb91e10fb3f99a127ed9c8be0.txt', '00004_jpg.rf.ceaaa39b5ce1bc929b07b407d6860953.txt', '00014_jpg.rf.b79345aaed5b59620b7e311442b1984e.txt', 'AightOne0832_jpg.rf.5fc534a2e5d81a873268ccb48207657e.txt', '00095_jpg.rf.692d5fe1fbf646d50159f132cdb1d1bb.txt', 'ThavantinDuo0848_jpg.rf.ec43bf91488eb30fc778fcd6effe3cde.txt', '20220721_161927_jpg.rf.16969a5b268932f601da501337901c9d.txt', 'ppe_0218_jpg.rf.dd36676e179a6be3830ea84301f490c3.txt', 'Video4_273_jpg.rf.ca58e5f6f92ec6750cfc10bab8794e8f.txt', 'download-27-_jpg.rf.412bf5a92170ccd5969c61b1387870e4.txt', 'Image_ipg-1958-_jpg.rf.40da7e26144bd8ea7e749ec5ff652127.txt', '00085_jpg.rf.0ee13de379a1dce820e28f19a3166e6c.txt', 'ppe_0961_jpg.rf.98c4a8fe10718fb9a2844cd64e03c6f2.txt', '20220721_161923_jpg.rf.da58dda6dfd4e9adb022c8643db2ae67.txt', 'ppe_0648_jpg.rf.bc36e78429000d3652023e7eb25734d5.txt', 'Aitin0036_jpg.rf.b20d74d6e8fd5a1a5f7a4a36f9e47788.txt', '9417812-serious-male-construction-worker-in-safety-vest-and-hard-hat_jpg.rf.73fe41f3bc70aaa4c741b55cd9c7458d.txt', 'ppe_0153_jpg.rf.f8fb327df38524164a96bd161e0f3f86.txt', 'ppe_0354_jpg.rf.48900e87fd6bf4ad9940e23b4141ef32.txt', '00126_jpg.rf.1971dcdda113ecd640024705b3eda7c5.txt', '00274_jpg.rf.19d58fa6b87d3e7f6a9ef095ad6df491.txt', '00271_jpg.rf.5b01646054d005e47fa45b1601b8621f.txt', 'images-29-_jpg.rf.3f2242320959d96fda0e52d8b41f8120.txt', 'AightDuo0193_jpg.rf.f29bb507697d23ec7db0c24d0ee57876.txt', 'images-2022-07-04T013441_jpg.rf.3339b26fc8648fbdb6ccd66f66aaa1f0.txt', '20220721_162229_jpg.rf.e5ce519cce9cfd36ab896365f4a76201.txt', 'image_62_jpg.rf.bcb3f6081938b452160cbbad58bcf136.txt', '51_JPG_jpg.rf.af2c775d8ef36e12b28bdcc365ee1eb7.txt', 'AightOne0167_jpg.rf.f4a21f0051b624c0def7de41b1271abd.txt', '00121_jpg.rf.e2569f1c2c5587f9bc623354ae8e6cb3.txt', 'Tan0207_jpg.rf.c8da58d1a2d3903fec1d75efc3434699.txt', 'KakaoTalk_20221018_001219713_18_jpg.rf.0c2745e3f57591cb9d799b0105731d53.txt', '00422_jpg.rf.66a2ca8bc031f46e62517105f3fa0b14.txt', 'images-2022-07-04T013101_jpg.rf.4e4990189326b2785c15ba10647c02ee.txt', 'Aitin4095_jpg.rf.be4542770492289674b1f7e1e2ea3ab8.txt', 'KakaoTalk_20221018_001219713_26_jpg.rf.0d4be2008723e1ce3f81ba6029a62026.txt', 'download-3-_jpeg_jpg.rf.02a726ce0f147035d40a68e44dec73c1.txt', 'KakaoTalk_20221018_001425789_06_jpg.rf.fd70da8486d22494235983d8ba375985.txt', '00125_jpg.rf.07c8dd851dfef10c05cd61514a585ba3.txt', 'ppe_0218_jpg.rf.4151a15d1a2cf7c16bc1583e29d4d354.txt', '20220621_132901_jpg.rf.09a3d2f28db56084d8cb295341c72f11.txt', 'ppe_0311_jpg.rf.dcc7dae5265f5ccbcc39ac3b09914836.txt', 'thumb0333_jpg.rf.73587db387d14da731168880c90b9a90.txt', 'thumb2443_jpg.rf.0054a22e31d5a195f1ed3ddfb17b28d4.txt', 'Tan0827827_jpg.rf.35672786a2d5aa13094b144c898fb26c.txt', 'images-2022-07-04T014014_jpg.rf.9f535e16462e848d77256efd50e9fd6c.txt', 'AightOne0138_jpg.rf.f6009d2596a77936f8ab5804508b787e.txt', 'Tan0150150_jpg.rf.21dbcae71da2ed5ed22d8fae7fd70c67.txt', 'images-2022-07-04T013948_jpg.rf.4d47baafd37588185bfd8d15c5c14c09.txt', 'ppe_0302_jpg.rf.1fcac53b74f17bf47cfaae3b3099039c.txt', 'thumb0454_jpg.rf.0138fa9e35c8740ee06b125b69a5d468.txt', '00629_jpg.rf.518aa45885c1f374fe7ee3b73735fb11.txt', 'image_96_jpg.rf.bc4aad3eda17025822c0670ac3ba95e6.txt', 'sic5242_jpg.rf.1c5bf41842d2c380a3a6add05ef5d382.txt', 'image_97_jpg.rf.f586c144181b016e84d3f09a986e5706.txt', '4843_RolePlay_ConstructionWorker_BoyGirl_jpg.rf.30a570e7ec2038ba2d30e2711e1fbe69.txt', 'Video4_274_jpg.rf.4963d1092721b6f8d76de470aa76259d.txt', 'image_53_jpg.rf.9118fa1e0faee7324b3df9e5c1230123.txt', 'KakaoTalk_20221018_001219713_18_jpg.rf.c77813984de67d78ec4128ad81a35408.txt', 'ppe_0374_jpg.rf.389e8e41774b83505767ab740cda17e9.txt', 'Aitin2428_jpg.rf.ca5ce4904cff178653a5294666386d3b.txt', 'images85_jpg.rf.87da82ea5d90067fb93c3ad2c2bb8d8c.txt', '00350_jpg.rf.b63b53649c0c6fcbf7179f333ace8b8a.txt', 'Aitin0937_jpg.rf.6f66b3b3451f8bb02078e5250b9e4479.txt', 'ppe_0274_jpg.rf.9c765f6413e15d0d46e7c149be8826c8.txt', 'image_96_jpg.rf.cda50f00bc9986728eab50ed5235346e.txt', 'ppe_0374_jpg.rf.3470c9435b2aad843c0c085e977c1698.txt', '20220721_162157_jpg.rf.0cf80eaba575f140e4b84c6767d03b48.txt', '00274_jpg.rf.0e26dd949e8bb93b097d7dba302461fc.txt', 'image_53_jpg.rf.e87c51dbd394de303a8b89ed5818b4f6.txt', 'Tan0365365_jpg.rf.00bd80dfa6579f96ff7bd1c312d4913a.txt', 'images237_jpg.rf.3b667f1244a25e2108065d6552234a60.txt', 'download-27-_jpg.rf.073ff6cfbfa355fe61bf867082faf963.txt', '101_JPG_jpg.rf.92641dc3bfda90109f2d2a3f5be95781.txt', 'Tan0639639_jpg.rf.33614f97b2046289c51f459367ba243e.txt', 'images-30-_jpg.rf.d91b215c164f9f97a28af45e766cbd35.txt', 'thumb0453_jpg.rf.36ce3cfc8e46c4417d957e47aae37489.txt', 'AightSird0207_jpg.rf.13c68e33e8172f3adadc561c4c7c6179.txt', 'images-2022-07-04T013055-1-_jpg.rf.91c723a41b8b93d9f99b81e9dd5301e2.txt', 'ppe_0771_jpg.rf.df0ff3556a570dc1a6e2f8e809c2f83e.txt', '95_JPG_jpg.rf.0a4d2e118896be9d806db8ddb19011e2.txt', 'Aitin2742_jpg.rf.e1cfc480a724af4d313c3112295995c8.txt', 'images-2022-07-04T013045_jpg.rf.35571925018f76d664af8c047a6f8c11.txt', '00014_jpg.rf.dbd86f7597d25f36766dd2bd1ab6095e.txt', 'ppe_1211_jpg.rf.b9f30ee8b3dfed7b21ff774911134456.txt', 'image_151_jpg.rf.607314ac1b165ba92eb83e3047d3a01b.txt', 'ppe_0153_jpg.rf.bb4cb7f5d855ba551bfda8f8f67d2969.txt', 'images63_jpg.rf.19a07d3f28c13e001482edae2bc46cdb.txt', '68_JPG_jpg.rf.19114e5270a0df784b3da152cdebd4c5.txt', '00122_jpg.rf.9036cddcbb176e79f21e258fc57e2e38.txt', 'li-lin-rZcC9J5ZT6I-unsplash_jpg.rf.2bfb636ce2379bede4435ea4efd355c4.txt', '00350_jpg.rf.131ac7d757335bdad7bad5a450caea44.txt', 'KakaoTalk_20221018_001219713_16_jpg.rf.1e4fc4df1a101adc5811737b6566a93e.txt', '51_JPG_jpg.rf.b5ba2155fc0fbd0925ade773a0d440ed.txt', 'images-50-_jpg.rf.2bafbe895fc3eade63418522ea0f1de5.txt', 'ppe_0873_jpg.rf.67517fd098a86b5aca0a119d0ce30b27.txt', 'download-1-_jpeg_jpg.rf.4874d5fcfe8e2965fdd60c5951ab4715.txt', 'image_150_jpg.rf.6b9ea08e9f5327613f7d63061009b94e.txt', 'Aitin2331_jpg.rf.a5d212d2d01abe198f11ca0ee5e953ab.txt', 'Aitin0937_jpg.rf.5cc6ad7eea6319d2021f64e55516766c.txt', '00137_jpg.rf.c7b774e79fe7979768ba9a1c985c56fe.txt', 'Tan0082_jpg.rf.a10134d5c23b8f3fcd0daf608f5ea01f.txt', '00486_jpg.rf.fa2cc8e526524d25b119b22eed5eee35.txt', 'Aitin2742_jpg.rf.1c67afc3b3ba20c0047e3a9a7407558f.txt', 'KakaoTalk_20221018_001312059_02_jpg.rf.4a1aec20a6dda4a59ed8478ed7a65502.txt', 'images49_jpg.rf.8bd6887e8033dde2d492684ac95ec4aa.txt', 'images-50-_jpg.rf.09d7ec3e8f9d1d0bd56739d09c021e3a.txt', 'images85_jpg.rf.8df20ddcc81f3b75a148b5ce9721a7c5.txt', 'images-2022-07-04T013435_jpg.rf.043ffca2a6d77842bf4786a5918eb106.txt', 'images-2022-07-04T013955_jpg.rf.024b4a838bdff435ba2e705ea6556307.txt', 'ppe_0961_jpg.rf.a17acf1a82a374f05eb5a178219382ba.txt', 'ppe_0546_jpg.rf.f8744f11fb512210682f52436140db12.txt', 'Worker-with-safety-glasses_6_jpeg_jpg.rf.27d9fce9c992ced8b97d5b2a15376e2b.txt', 'image_4_jpg.rf.6ea6d9e745ba57a8b249e4c6347df03e.txt', 'image_153_jpg.rf.43826bc683f4fb8a60765b5851783ffb.txt', 'Aitin2742_jpg.rf.6537200822fde884749c08ccaa58ec77.txt', 'images-34-_jpg.rf.15dde987e90301893be022fabe0a055d.txt', 'Aitin0001_jpg.rf.642639bf88b1b4ae53b3e14bdbb10d5e.txt', 'gettyimages-650169013-612x612_jpg.rf.ca1605dba29be6920384d2009441d9bc.txt', 'images-2022-07-04T013045_jpg.rf.074fe39fec5ae80af9915238d2b07bc4.txt', 'images-34-_jpg.rf.3726101c0f8ccd2106118cd73b390b63.txt', 'images-2022-07-04T013958_jpg.rf.3c2266829cb0473857b18b78f4fe5c84.txt', 'construction-worker1_grande_jpg.rf.fa62d0465d2179cab25ea12d37c0482c.txt', '20220721_162157_jpg.rf.5a1b24822fc2c77e3e18f624c2289996.txt', '00212_jpg.rf.582a67c0ca4eaf8c1e37fafb09e51bdb.txt', 'image_70_jpg.rf.786b9f2d4d5fb89e9118282e36188066.txt', 'thumb2444_jpg.rf.bd8ec5fe10c18ded4fc2df6390e99dc7.txt', '00060_jpg.rf.cd4b0a089c92f916228e9cd0143447a9.txt', 'ppe_1207_jpg.rf.cab304c988ef29a8fde0125a17d2f97d.txt', '00096_jpg.rf.6e33e05e149f4ef82004a85a085b9e36.txt', 'ppe_0218_jpg.rf.7557e839d9c985dbe2b79430b69b6348.txt', 'gettyimages-632170387-612x612_jpg.rf.bd8ec50ff56a126da57e7443030fa513.txt', '20220721_162848_jpg.rf.8c29c48ec64b8ed5a0d50dcdaf3b498e.txt', 'ppe_0274_jpg.rf.4e88d83fcd9beb25fe34bcd73a4771fa.txt', '00087_jpg.rf.377cf21010d81d8a1e86d8a7b26de620.txt', 'Aitin0586_jpg.rf.f0c8dd6604a12970fb9260673ba61558.txt', 'Ear-protection_5_jpeg_jpg.rf.f1536551ded60572c53c7123912ab34d.txt', 'thumb2435_jpg.rf.e0e11502cfbcebcff8f20db411c9ae77.txt', '00236_jpg.rf.8b9218814fba6ab93cfd814cbb45f1d4.txt', 'ppe_0307_jpg.rf.777f894691af83e13da24cc2026398c2.txt', 'Aitin0150_jpg.rf.948cff79061a6936e6650d517f14c695.txt', 'ppe_0771_jpg.rf.ff35c81b2fe33757a98b7bb1cb615b76.txt', 'Aitin2428_jpg.rf.a3ccd7c7ffb9b3c2435c62dc2a7f7ac1.txt', 'Ear-protection_14_jpeg_jpg.rf.a955f51831248185e700875c6177db0d.txt', '00350_jpg.rf.8b6c3d41ca69e29b2833be7c7c4addb1.txt', '98_JPG_jpg.rf.0fdb917ec7108fe07a65a13b052737ef.txt', '00060_jpg.rf.8454996f2ead80567c7b1b1c0777e005.txt', '20220721_162229_jpg.rf.766fdc6bfe1f6f23dd1876dceb0e5473.txt', 'ppe_1211_jpg.rf.366a0b949c2f475bca37312b0ec67dde.txt', 'Aitin1788_jpg.rf.14375e679fbdaf0a9133a464632024d1.txt', '00176_jpg.rf.9de173b652c4eb79ad748c9b82a18484.txt', 'image_27_jpg.rf.1e9d95b619f6685a642c85af0b32cfe0.txt', 'gettyimages-1036354066-612x612_jpg.rf.15d8e6522e649407b2f50f539e168bc9.txt', 'Tan0745745_jpg.rf.f09126526d369aab7fbebe2c86b556fc.txt', 'Ear-protection_5_jpeg_jpg.rf.9845f4b0320e80110cc691439f3015fe.txt', 'Tan0972972_jpg.rf.5a20a9d9ca3e9c56e5e76c955de979b0.txt', 'Image_ipg-1381-_jpg.rf.4a975bdc4f0c2134682f14872abc45d3.txt', 'Tan0827827_jpg.rf.fa4e4368247443e9714a96d67e7a05fc.txt', 'image_62_jpg.rf.abf39e35d581bdb1962c99b1ec28ad62.txt', 'AightSird0542_jpg.rf.da9a1622ee4d4b4beacfbdba236e0a42.txt', 'AightSird0207_jpg.rf.b7e351c9a0dd9461a0f27ad3507d837b.txt', 'image_95_jpg.rf.c947fb94923989c3d5710ac0aaa198f9.txt', 'ppe_0636_jpg.rf.10a35cf4414b4962b4257e5d19a67f3d.txt', '00236_jpg.rf.27fbdb231773faf7dd7e9d535a289e5f.txt', 'Aitin3852_jpg.rf.c1d10b6023afca6c2acbbd3b0b3b54ff.txt', 'images49_jpg.rf.1fdd948f82293efb0cdb225e5f42ffc7.txt', 'KakaoTalk_20221018_001219713_15_jpg.rf.c513c77b559cda5cb774a87cb962ff12.txt', 'AightOne0317_jpg.rf.61eb8e8bf70b783b89f67669a756ae49.txt', '95_JPG_jpg.rf.7d3c2141f05e63c17837a3db8c6eff4a.txt', 'construction-crack-146741_jpg.rf.7c683e60b0673d636175f88cb53bd751.txt', 'images-2022-07-04T013506_jpg.rf.616716d73e83d2f6e6d7ba61ff772889.txt', '20220721_162157_jpg.rf.2decaa9a04756ff87fbda0f5ee81eb93.txt', 'Image_ipg-2083-_jpg.rf.2de92817dc35ba6a98870b0a5c95ff4c.txt', '00100_jpg.rf.e10bd910a5b938d51e9c154de67d3b58.txt', 'image_27_jpg.rf.448b6255132790403da80ea27f03ff21.txt', 'sic2817_jpg.rf.5d820c9950ff1895cda46fb58ccb38dc.txt', 'KakaoTalk_20221018_001219713_24_jpg.rf.b670fc8eccfd83b8919bc05777a9eb67.txt', 'KakaoTalk_20221018_001425789_06_jpg.rf.1118e205d00e3002b7f0e57ef8846b46.txt', '00254_jpg.rf.9f2047c0496e33024325272f6e5d077c.txt', '00629_jpg.rf.32f4472018b8d2428d95fed81f860f13.txt', 'Safety-goggles_6_jpeg_jpg.rf.059a0aeb941474bcf4563cf4537aba30.txt', 'images-2022-07-04T013055-1-_jpg.rf.0c3d860e94d5c37e836e27502c59819e.txt', '20220721_161927_jpg.rf.e379d350eb5b76a29828671028e72348.txt']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "166ef080"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous output showed that the `train` directory contains `images` and `labels` subdirectories, with `.jpg` files in `images` and corresponding `.txt` files in `labels`. To understand the annotation format and identified PPE categories, the next step is to examine the content of one of these `.txt` label files. I will read and display the content of one sample label file to infer the annotation format and PPE categories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7379dff2",
        "outputId": "48b7ca7a-5623-44fc-db95-ce19168fbcc9"
      },
      "source": [
        "label_file_path = 'train/labels/00225_jpg.rf.e8373c3ded637a026b4ce0f51a2f181f.txt'\n",
        "\n",
        "print(f\"Content of '{label_file_path}':\")\n",
        "with open(label_file_path, 'r') as f:\n",
        "    print(f.read())\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content of 'train/labels/00225_jpg.rf.e8373c3ded637a026b4ce0f51a2f181f.txt':\n",
            "7 0.48828125 0.565625 0.7875 0.575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12452cc7"
      },
      "source": [
        "The inspection of the `train` directory revealed the following structure:\n",
        "\n",
        "- The `train` directory contains two subdirectories: `images` and `labels`.\n",
        "- The `images` subdirectory contains image files (e.g., `.jpg`).\n",
        "- The `labels` subdirectory contains corresponding annotation files (e.g., `.txt`).\n",
        "\n",
        "Examination of a sample label file (`train/labels/00225_jpg.rf.e8373c3ded637a026b4ce0f51a2f181f.txt`) showed the content: `7 0.48828125 0.565625 0.7875 0.575`.\n",
        "\n",
        "This format is consistent with the YOLO object detection annotation format, where each line represents an object with:\n",
        "1.  **Class ID**: The first number (`7`) represents the category of the detected object.\n",
        "2.  **Normalized Bounding Box Coordinates**: The subsequent four numbers (`0.48828125 0.565625 0.7875 0.575`) represent the `x_center`, `y_center`, `width`, and `height` of the bounding box, all normalized to the image's dimensions (values between 0 and 1).\n",
        "\n",
        "To fully understand the dataset and identify the *available PPE categories*, a mapping from these numerical `Class IDs` to human-readable class names (e.g., `7` -> 'Hardhat') is required. This information is typically found in a `classes.txt` or `data.yaml` file. However, such a file was not found within the extracted `train` directory or its parent directory during the exploration. Without this mapping, the specific PPE categories represented by the class IDs cannot be determined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "449868a3"
      },
      "source": [
        "## Install Necessary Libraries\n",
        "\n",
        "### Subtask:\n",
        "Install required Python libraries for object detection (`ultralytics` for YOLOv8) and text-to-speech conversion (`gTTS`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1679b8f"
      },
      "source": [
        "**Reasoning**:\n",
        "To complete the subtask of installing the required libraries, I will use `pip install` commands to install `ultralytics` and `gTTS` in a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "f0f86d18",
        "outputId": "a106647b-7dbe-4bf9-8888-56612b1dc9f3"
      },
      "source": [
        "pip install ultralytics gTTS\n",
        "print(\"Libraries 'ultralytics' and 'gTTS' installed successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-35980888.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-35980888.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install ultralytics gTTS\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97974364"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to install libraries failed because `pip install` was not prefixed with `!` to execute it as a shell command in a Colab environment. I will add the `!` prefix to correctly install `ultralytics` and `gTTS`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6b5cbcb",
        "outputId": "379bbd22-43bc-4f0a-c7bd-aa652c737f48"
      },
      "source": [
        "get_ipython().system('pip install ultralytics gTTS')\n",
        "print(\"Libraries 'ultralytics' and 'gTTS' installed successfully.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.4.14-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.13.0.92)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cpu)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting click<8.2,>=7.1 (from gTTS)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (26.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2026.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.21.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.4.14-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: click, gTTS, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.1\n",
            "    Uninstalling click-8.3.1:\n",
            "      Successfully uninstalled click-8.3.1\n",
            "Successfully installed click-8.1.8 gTTS-2.5.4 ultralytics-8.4.14 ultralytics-thop-2.0.18\n",
            "Libraries 'ultralytics' and 'gTTS' installed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7269827"
      },
      "source": [
        "## Prepare Dataset for YOLOv8 Training\n",
        "\n",
        "### Subtask:\n",
        "Convert the inspected dataset into a format compatible with YOLOv8. This involves defining class names, creating a data.yaml configuration file, and splitting the dataset into training and validation sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f07b87b"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step in preparing the dataset for YOLOv8 training is to define the class names and create the `data.yaml` configuration file. This file is crucial for YOLOv8 to understand the dataset structure and the classes it needs to detect. I will use the assumed class names and specify the paths for the training and validation data that will be created in subsequent steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65bc64b4",
        "outputId": "2e98c41d-c38e-4139-cee3-e43dc1584116"
      },
      "source": [
        "import yaml\n",
        "\n",
        "# Define the assumed class names based on common PPE items.\n",
        "# The class ID '7' observed in the label file will be mapped to 'goggles' for this example.\n",
        "class_names = [\n",
        "    'hardhat',   # 0\n",
        "    'vest',      # 1\n",
        "    'gloves',    # 2\n",
        "    'boots',     # 3\n",
        "    'person',    # 4\n",
        "    'earmuffs',  # 5\n",
        "    'mask',      # 6\n",
        "    'goggles'    # 7 (based on the sample label file)\n",
        "]\n",
        "\n",
        "# Create the data.yaml content\n",
        "data_yaml_content = {\n",
        "    'path': '/content/', # Dataset root directory\n",
        "    'train': 'train_split/images', # Train images relative to 'path'\n",
        "    'val': 'val_split/images',   # Val images relative to 'path'\n",
        "    'names': class_names\n",
        "}\n",
        "\n",
        "# Define the path for data.yaml\n",
        "data_yaml_path = 'data.yaml'\n",
        "\n",
        "# Write the content to data.yaml\n",
        "with open(data_yaml_path, 'w') as f:\n",
        "    yaml.dump(data_yaml_content, f, default_flow_style=False)\n",
        "\n",
        "print(f\"'{data_yaml_path}' created successfully with content:\")\n",
        "print(yaml.dump(data_yaml_content, default_flow_style=False))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'data.yaml' created successfully with content:\n",
            "names:\n",
            "- hardhat\n",
            "- vest\n",
            "- gloves\n",
            "- boots\n",
            "- person\n",
            "- earmuffs\n",
            "- mask\n",
            "- goggles\n",
            "path: /content/\n",
            "train: train_split/images\n",
            "val: val_split/images\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbb0d2af"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `data.yaml` file has been created, the next step in preparing the dataset for YOLOv8 training is to split the dataset into training and validation sets. This involves creating the necessary directory structure for the splits and then moving the image and label files from the original `train` directory into these new directories, maintaining a consistent split ratio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91208a7d",
        "outputId": "d1e40bc0-5f44-4e3f-dd8b-c0377c430a38"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define paths\n",
        "original_images_dir = 'train/images'\n",
        "original_labels_dir = 'train/labels'\n",
        "\n",
        "train_images_dir = 'train_split/images'\n",
        "train_labels_dir = 'train_split/labels'\n",
        "val_images_dir = 'val_split/images'\n",
        "val_labels_dir = 'val_split/labels'\n",
        "\n",
        "# Create target directories if they don't exist\n",
        "for d in [train_images_dir, train_labels_dir, val_images_dir, val_labels_dir]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# Get all image files\n",
        "image_files = [f for f in os.listdir(original_images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "random.shuffle(image_files)\n",
        "\n",
        "# Define split ratio (e.g., 80% train, 20% validation)\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(image_files) * split_ratio)\n",
        "\n",
        "train_files = image_files[:split_index]\n",
        "val_files = image_files[split_index:]\n",
        "\n",
        "print(f\"Total images: {len(image_files)}\")\n",
        "print(f\"Training images: {len(train_files)}\")\n",
        "print(f\"Validation images: {len(val_files)}\")\n",
        "\n",
        "# Function to move files\n",
        "def move_files(file_list, source_image_dir, source_label_dir, dest_image_dir, dest_label_dir):\n",
        "    for filename in file_list:\n",
        "        # Move image\n",
        "        shutil.copy(os.path.join(source_image_dir, filename), os.path.join(dest_image_dir, filename))\n",
        "\n",
        "        # Move corresponding label (assuming same base name, .txt extension)\n",
        "        label_filename = os.path.splitext(filename)[0] + '.txt'\n",
        "        if os.path.exists(os.path.join(source_label_dir, label_filename)):\n",
        "            shutil.copy(os.path.join(source_label_dir, label_filename), os.path.join(dest_label_dir, label_filename))\n",
        "        else:\n",
        "            print(f\"Warning: Label file {label_filename} not found for image {filename}\")\n",
        "\n",
        "# Move files to train and validation directories\n",
        "print(\"Moving training files...\")\n",
        "move_files(train_files, original_images_dir, original_labels_dir, train_images_dir, train_labels_dir)\n",
        "\n",
        "print(\"Moving validation files...\")\n",
        "move_files(val_files, original_images_dir, original_labels_dir, val_images_dir, val_labels_dir)\n",
        "\n",
        "print(\"Dataset split into training and validation sets successfully.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 368\n",
            "Training images: 294\n",
            "Validation images: 74\n",
            "Moving training files...\n",
            "Moving validation files...\n",
            "Dataset split into training and validation sets successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddfc1b95"
      },
      "source": [
        "## Train YOLOv8 Model for PPE Detection\n",
        "\n",
        "### Subtask:\n",
        "Initialize a pre-trained YOLOv8 object detection model and fine-tune it on your prepared PPE dataset. This will involve setting training parameters like the number of epochs, batch size, and learning rate to achieve optimal performance for detecting gloves, vests, boots, goggles, and helmets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f37f309b"
      },
      "source": [
        "**Reasoning**:\n",
        "To initialize and train the YOLOv8 model, I will import the `YOLO` class from `ultralytics` and load the `yolov8n.pt` pre-trained model. Then, I will start the training process by calling the `.train()` method, passing the `data.yaml` configuration, the number of epochs, and the image size as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "189a4513",
        "outputId": "74411e1b-e987-473c-8adc-5b97c7d8506f"
      },
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a pre-trained YOLOv8n model\n",
        "model = YOLO('yolov8n.pt')  # You can choose other models like yolov8s.pt, yolov8m.pt, etc.\n",
        "\n",
        "# Train the model\n",
        "# Set the number of epochs, image size, and data configuration file.\n",
        "results = model.train(data='data.yaml', epochs=50, imgsz=640)\n",
        "\n",
        "print(\"YOLOv8 model training initiated successfully.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file  \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8n.pt to 'yolov8n.pt': 100%  6.2MB 108.7MB/s 0.1s\n",
            "Ultralytics 8.4.14  Python-3.12.12 torch-2.9.0+cpu CPU (Intel Xeon CPU @ 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100%  755.1KB 28.8MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, 16, None, [64, 128, 256]] \n",
            "Model summary: 130 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 1346.2453.4 MB/s, size: 79.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/train_split/labels... 294 images, 0 backgrounds, 0 corrupt: 100%  294/294 1.6Kit/s 0.2s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/train_split/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 1634.2389.0 MB/s, size: 56.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/val_split/labels... 74 images, 0 backgrounds, 0 corrupt: 100%  74/74 2.2Kit/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/val_split/labels.cache\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Plotting labels to /content/runs/detect/train/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/50         0G      1.464       3.73      1.563         32        640: 100%  19/19 11.9s/it 3:47\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.5s/it 22.4s\n",
            "                   all         74        277     0.0182      0.485      0.106     0.0583\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/50         0G      1.363      2.637      1.481         28        640: 100%  19/19 11.9s/it 3:46\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.0s/it 21.1s\n",
            "                   all         74        277     0.0213      0.595      0.181      0.103\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/50         0G      1.377      2.046       1.41         33        640: 100%  19/19 11.8s/it 3:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.0s/it 21.1s\n",
            "                   all         74        277        0.9      0.147       0.31      0.192\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/50         0G      1.343      1.847      1.395         51        640: 100%  19/19 11.8s/it 3:43\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.1s/it 21.2s\n",
            "                   all         74        277      0.855      0.218      0.345      0.193\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/50         0G      1.278      1.736      1.397         33        640: 100%  19/19 11.5s/it 3:38\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.1s/it 21.2s\n",
            "                   all         74        277      0.761      0.324      0.378      0.222\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/50         0G      1.297      1.657      1.354         46        640: 100%  19/19 11.4s/it 3:37\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.3s/it 21.8s\n",
            "                   all         74        277      0.689       0.35      0.358      0.218\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/50         0G      1.231      1.533      1.328         59        640: 100%  19/19 11.6s/it 3:40\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.0s/it 21.1s\n",
            "                   all         74        277      0.746      0.396      0.423      0.261\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/50         0G      1.222      1.499      1.306         43        640: 100%  19/19 11.6s/it 3:40\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.0s/it 21.0s\n",
            "                   all         74        277      0.693       0.36      0.388      0.237\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/50         0G      1.196      1.431      1.281         49        640: 100%  19/19 11.5s/it 3:38\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.8s/it 20.3s\n",
            "                   all         74        277      0.781      0.393      0.448      0.276\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/50         0G      1.152      1.346       1.26         30        640: 100%  19/19 11.7s/it 3:43\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.1s/it 21.2s\n",
            "                   all         74        277      0.739      0.451      0.477      0.288\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/50         0G      1.125      1.317      1.268         65        640: 100%  19/19 11.6s/it 3:41\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.0s/it 20.9s\n",
            "                   all         74        277      0.795      0.431      0.493      0.303\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/50         0G      1.123      1.253       1.24         33        640: 100%  19/19 11.8s/it 3:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.0s/it 21.0s\n",
            "                   all         74        277      0.824      0.448      0.518      0.318\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/50         0G      1.109      1.217      1.242         55        640: 100%  19/19 11.7s/it 3:43\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.0s/it 20.9s\n",
            "                   all         74        277      0.809      0.478      0.556      0.355\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/50         0G        1.1      1.213       1.22         50        640: 100%  19/19 11.7s/it 3:43\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.0s/it 20.9s\n",
            "                   all         74        277      0.664      0.468      0.571       0.35\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/50         0G      1.047      1.173      1.213         56        640: 100%  19/19 11.8s/it 3:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.9s/it 20.7s\n",
            "                   all         74        277      0.689      0.649      0.688      0.453\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/50         0G      1.023      1.142      1.189         42        640: 100%  19/19 11.8s/it 3:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.9s/it 20.7s\n",
            "                   all         74        277      0.756      0.636      0.713      0.493\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/50         0G      1.051      1.156      1.203         39        640: 100%  19/19 11.8s/it 3:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.9s/it 20.6s\n",
            "                   all         74        277      0.758      0.629      0.716      0.492\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/50         0G      1.025       1.06      1.194         31        640: 100%  19/19 11.7s/it 3:43\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.0s/it 20.9s\n",
            "                   all         74        277      0.871      0.565      0.741      0.445\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/50         0G      1.012      1.106      1.194         27        640: 100%  19/19 11.7s/it 3:43\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.7s/it 20.0s\n",
            "                   all         74        277      0.783      0.646       0.72      0.492\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/50         0G      1.002      1.041      1.169         50        640: 100%  19/19 11.8s/it 3:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.7s/it 20.0s\n",
            "                   all         74        277      0.927      0.607      0.754       0.53\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/50         0G     0.9846      1.055      1.167         58        640: 100%  19/19 11.8s/it 3:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.8s/it 20.3s\n",
            "                   all         74        277      0.869      0.671      0.733      0.512\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/50         0G     0.9979      1.075      1.167         28        640: 100%  19/19 11.9s/it 3:45\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.2s/it 21.7s\n",
            "                   all         74        277      0.888      0.667      0.766      0.517\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/50         0G      0.939          1      1.131         70        640: 100%  19/19 11.8s/it 3:45\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.1s/it 21.2s\n",
            "                   all         74        277      0.882      0.605      0.788      0.543\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/50         0G     0.9343     0.9659      1.138         41        640: 100%  19/19 11.7s/it 3:43\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.2s/it 21.6s\n",
            "                   all         74        277      0.726      0.768      0.831      0.574\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/50         0G      0.955     0.9429      1.148         62        640: 100%  19/19 11.7s/it 3:42\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.8s/it 20.5s\n",
            "                   all         74        277      0.896       0.63      0.914      0.548\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/50         0G      0.929     0.9512      1.117         26        640: 100%  19/19 11.9s/it 3:46\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.1s/it 21.3s\n",
            "                   all         74        277      0.578      0.802      0.834       0.54\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/50         0G     0.9274     0.9064       1.12         37        640: 100%  19/19 11.8s/it 3:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.1s/it 21.4s\n",
            "                   all         74        277      0.579      0.842      0.862      0.565\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/50         0G     0.9144     0.9119      1.131         39        640: 100%  19/19 11.7s/it 3:42\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.0s/it 21.0s\n",
            "                   all         74        277      0.623       0.85      0.901      0.612\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/50         0G     0.8948     0.8927      1.094         28        640: 100%  19/19 11.7s/it 3:41\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.9s/it 20.8s\n",
            "                   all         74        277      0.749      0.883      0.927      0.649\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/50         0G      0.878     0.8299      1.082         52        640: 100%  19/19 11.8s/it 3:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.8s/it 20.5s\n",
            "                   all         74        277       0.92       0.81      0.935       0.66\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      31/50         0G     0.8518      0.847      1.078         66        640: 100%  19/19 11.7s/it 3:43\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.9s/it 20.7s\n",
            "                   all         74        277      0.872      0.708      0.769      0.563\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      32/50         0G     0.8791      0.875      1.091         33        640: 100%  19/19 11.8s/it 3:45\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.8s/it 20.4s\n",
            "                   all         74        277      0.876      0.758      0.858       0.63\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      33/50         0G     0.8331     0.7992      1.079         61        640: 100%  19/19 11.7s/it 3:43\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.8s/it 20.5s\n",
            "                   all         74        277      0.844      0.768      0.893      0.688\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      34/50         0G     0.8375     0.8153      1.075         52        640: 100%  19/19 11.8s/it 3:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.9s/it 20.6s\n",
            "                   all         74        277      0.913       0.75      0.884      0.664\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      35/50         0G     0.8586      0.837      1.085         20        640: 100%  19/19 11.8s/it 3:43\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.0s/it 21.0s\n",
            "                   all         74        277      0.859      0.758      0.854      0.669\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      36/50         0G     0.8147     0.7687      1.068         48        640: 100%  19/19 11.8s/it 3:45\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.7s/it 20.1s\n",
            "                   all         74        277      0.882      0.729      0.865      0.662\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      37/50         0G     0.8167     0.7828      1.063         29        640: 100%  19/19 11.7s/it 3:43\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.9s/it 20.7s\n",
            "                   all         74        277      0.877       0.78      0.877      0.681\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      38/50         0G     0.7906     0.7632      1.057         48        640: 100%  19/19 11.8s/it 3:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.0s/it 21.0s\n",
            "                   all         74        277      0.886        0.8       0.88      0.664\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      39/50         0G     0.7755     0.7391       1.04         50        640: 100%  19/19 11.7s/it 3:43\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.8s/it 20.5s\n",
            "                   all         74        277       0.88      0.788       0.88      0.704\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      40/50         0G     0.7812     0.7349      1.043         51        640: 100%  19/19 11.8s/it 3:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.8s/it 20.3s\n",
            "                   all         74        277      0.887      0.774      0.881      0.705\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      41/50         0G     0.7745     0.8006      1.044         34        640: 100%  19/19 11.7s/it 3:42\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.0s/it 21.0s\n",
            "                   all         74        277      0.872      0.732      0.891      0.701\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      42/50         0G     0.7567     0.8095      1.042         29        640: 100%  19/19 11.8s/it 3:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.1s/it 21.4s\n",
            "                   all         74        277      0.883      0.737      0.873      0.677\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      43/50         0G     0.7421     0.7433       1.03         44        640: 100%  19/19 11.8s/it 3:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.7s/it 20.1s\n",
            "                   all         74        277      0.862      0.786      0.867      0.682\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      44/50         0G     0.7229     0.7245      1.026         21        640: 100%  19/19 11.7s/it 3:41\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.7s/it 20.2s\n",
            "                   all         74        277      0.856      0.799      0.865      0.675\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      45/50         0G     0.6818     0.7155      1.008         17        640: 100%  19/19 11.4s/it 3:37\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.6s/it 19.9s\n",
            "                   all         74        277      0.851      0.817      0.868      0.677\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      46/50         0G     0.6958      0.689     0.9905         25        640: 100%  19/19 11.7s/it 3:42\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.9s/it 20.7s\n",
            "                   all         74        277       0.88       0.79      0.864      0.696\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      47/50         0G      0.689     0.6705     0.9932         32        640: 100%  19/19 11.6s/it 3:40\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.5s/it 19.4s\n",
            "                   all         74        277      0.877      0.787      0.858      0.692\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      48/50         0G     0.6944     0.6657     0.9812         49        640: 100%  19/19 11.6s/it 3:40\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.7s/it 20.1s\n",
            "                   all         74        277      0.879      0.802      0.862       0.69\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      49/50         0G      0.676     0.6865     0.9899         18        640: 100%  19/19 11.7s/it 3:42\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 7.0s/it 21.1s\n",
            "                   all         74        277      0.893      0.795      0.864      0.694\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      50/50         0G     0.6652     0.6706     0.9763         12        640: 100%  19/19 11.6s/it 3:40\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.6s/it 19.9s\n",
            "                   all         74        277      0.891      0.802      0.867      0.696\n",
            "\n",
            "50 epochs completed in 3.385 hours.\n",
            "Optimizer stripped from /content/runs/detect/train/weights/last.pt, 6.3MB\n",
            "Optimizer stripped from /content/runs/detect/train/weights/best.pt, 6.3MB\n",
            "\n",
            "Validating /content/runs/detect/train/weights/best.pt...\n",
            "Ultralytics 8.4.14  Python-3.12.12 torch-2.9.0+cpu CPU (Intel Xeon CPU @ 2.20GHz)\n",
            "Model summary (fused): 73 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  3/3 6.3s/it 18.9s\n",
            "                   all         74        277      0.888      0.775      0.886      0.708\n",
            "               hardhat         20         46       0.85      0.717      0.819      0.588\n",
            "                  vest          1          1      0.809          1      0.995      0.995\n",
            "                gloves          2          2      0.789        0.5      0.695      0.496\n",
            "                person         55         89          1      0.843      0.966      0.728\n",
            "                  mask         32         51      0.905      0.744      0.871      0.681\n",
            "               goggles         47         88      0.974      0.847      0.969      0.759\n",
            "Speed: 2.7ms preprocess, 238.6ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "YOLOv8 model training initiated successfully.\n"
          ]
        }
      ]
    }
  ]
}